{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL database!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harsh\\AppData\\Local\\Temp\\ipykernel_19096\\3579922914.py:25: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data fetched successfully!\n",
      "0    TMC to deploy Shatrughan Sinha to campaign for...\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "import pandas as pd\n",
    "\n",
    "# Set up the MySQL connection\n",
    "db_config = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"user\": \"root\",\n",
    "    \"password\": \"12345678\",\n",
    "    \"database\": \"news_db\"\n",
    "}\n",
    "\n",
    "# Connect to the database\n",
    "try:\n",
    "    connection = mysql.connector.connect(**db_config)\n",
    "    print(\"Connected to MySQL database!\")\n",
    "except mysql.connector.Error as e:\n",
    "    print(f\"Error connecting to MySQL: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Define your SQL query\n",
    "query = \"SELECT * FROM news order by id desc LIMIT 20;\"  \n",
    "\n",
    "# Fetch data into a pandas DataFrame\n",
    "try:\n",
    "    df = pd.read_sql(query, connection)\n",
    "    print(\"Data fetched successfully!\")\n",
    "    print(df.head(1)[\"title\"])  \n",
    "except Exception as e:\n",
    "    print(f\"Error fetching data: {e}\")\n",
    "\n",
    "# Close the connection\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary: Ochre Coloured Pottery culture dates back to c.â€‰1900-1300 BCE .\n",
      "Archaeologists consider it as a Late Harappan expansion and archaeological continuity of the previous Bara style .\n"
     ]
    }
   ],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = BartForConditionalGeneration.from_pretrained(\"./fine_tuned_bart\")\n",
    "tokenizer = BartTokenizer.from_pretrained(\"./fine_tuned_bart\")\n",
    "\n",
    "# Test the model with a sample input\n",
    "input_text = \n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\",\n",
    "                   max_length=1024, truncation=True)\n",
    "\n",
    "# Generate summary\n",
    "summary_ids = model.generate(\n",
    "    inputs[\"input_ids\"], max_length=128, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Generated Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Content-Summary_CleanData.csv\")\n",
    "df.head()\n",
    "text1 = df.iloc[0][\"Content\"]\n",
    "summary1 = df.iloc[0][\"Summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores:\n",
      "ROUGE-1: Score(precision=0.21621621621621623, recall=0.35555555555555557, fmeasure=0.2689075630252101)\n",
      "ROUGE-2: Score(precision=0.0410958904109589, recall=0.06818181818181818, fmeasure=0.05128205128205127)\n",
      "ROUGE-L: Score(precision=0.10810810810810811, recall=0.17777777777777778, fmeasure=0.13445378151260504)\n"
     ]
    }
   ],
   "source": [
    "from rouge_score import rouge_scorer\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "fine_tuned_model_path = \"./fine_tuned_bart\"\n",
    "model = BartForConditionalGeneration.from_pretrained(fine_tuned_model_path)\n",
    "tokenizer = BartTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "\n",
    "# Function to generate summary using your fine-tuned model\n",
    "\n",
    "\n",
    "def generate_summary_fine_tuned(text, max_length=100, min_length=25):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\",\n",
    "                       max_length=1024, truncation=True)\n",
    "    summary_ids = model.generate(\n",
    "        inputs[\"input_ids\"], max_length=max_length, min_length=min_length)\n",
    "    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# Example of some test data\n",
    "original_text = text1\n",
    "reference_summary = summary1\n",
    "\n",
    "# Generate summary from the fine-tuned model\n",
    "generated_summary = generate_summary_fine_tuned(original_text)\n",
    "\n",
    "# Create a ROUGE scorer\n",
    "scorer = rouge_scorer.RougeScorer(\n",
    "    ['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "# Compute ROUGE score\n",
    "scores = scorer.score(reference_summary, generated_summary)\n",
    "\n",
    "# Display the results\n",
    "print(\"ROUGE Scores:\")\n",
    "print(f\"ROUGE-1: {scores['rouge1']}\")\n",
    "print(f\"ROUGE-2: {scores['rouge2']}\")\n",
    "print(f\"ROUGE-L: {scores['rougeL']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of metrics commonly used to evaluate the quality of automatically generated text, such as machine-generated summaries, by comparing them to reference (human-created) summaries. It measures the overlap between the generated and reference summaries at different levels (e.g., words, phrases, or sequences).\n",
    "\n",
    "# It is widely used in text summarization tasks to assess how well a model captures the important information from the source content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
